
\section{Logical layer}
This layer represents the pure software part.\\
Logical encoding may try to overcome some of the previous limitations.\\
Standard for VLC is encoding in Manchester Code, also some kind of protocol could attempt to do bit recovery.

\todo{redo this figure with a more recent transmission}
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[height=150px]{img/sample} 
   \caption{Example of the results of interpreting digital information from analog input.}
   \label{fig:sample}
\end{figure}
 \subsection{Reading the signal}
[ shape, noise, machine learning difficulties, final resolution ]\\
A key part of this project is to convert analog sensor data representing light variations into binary code.
Fig. \ref{fig:sample} shows a sample of transmission. The green line represents the original values from the light sensor, while the purple line is a reconstruction of what the digital transmission would look like. Manchester encoding allows transmission to only have two kinds of peaks, either representing single bits, like a 1 or a 0, or two bits of the same kind.
Having a longer sequence of identical symbols would be impossible with this encoding.\\

Multiple attempts have been made to design an algorithm that would interpret this sequences of values into binary data.\\
The first version of it made use of Machine Learning techniques to train a model with training data, "teaching" a classifier what kinds of data and results one would expect in this application. The data was taken in different circumstances of ambient light interference, with different messages sent and at different times.
Multiple classifiers were tested, with a maximum success rate of about 75\%. The classifiers included NaiveBayes, RandomForest, AdaBoost and ArtificialNeuralNetworks, all with comparable results.\\
There were major difficulties with this approach. For starters, there were problems identifying the labels to train these models, namely the classes or categories where the data would belong.
Initially, the four categories were "1", "0", "11", and "00". The basic idea is that given a sequence of values it should fall in one of these four classes. The problem was though that single bits and double bits have different sizes in amount of values that form them. This is also called the number of features.\\
Most standard modules for machine learning require the same number of features for all the labels.\\
In order to get around this rule, the labels became "10", "01", "11", and "00". Eventually, also these labels were produced by different numbers of features. Other combinations were tried, and eventually all leading to the same result. 
One way to get around this problem was to have a fixed number of features, but big enough to include all the possibilities. Samples smaller than this maximum amount (virtually all of the samples) would be filled up with blank values until they reached the intended size.\\
Another way would be to train a classifier for each specific number of features that may occur. In this case, the amount of classifiers needed would be around 4, for number of features ranging between 7 and 10.
The downside of this approach is that each classifier becomes very specific to a restricted set of the training data, potentially loosing track completely of certain labels.\\
The final setup for machine learning prediction used the labels "101", "100", "011", "110", "001", "11", "00", "10", "01", scaling of the features and larger sample size to be filled up with blanks. This produced overall the best results for the machine learning approach, but never above 80\% success rate.\\
A second downside to this approach would be that when receiving values from the sensor in real time, one would need to 
keep a moving window  of the size of the number of features and try to obtain a prediction from the classifier each time.
This might likely slow down the reading process.\\

These poor results ultimately led to a change of approach for reading the data. 
Just looking at fig. \ref{sample} it's pretty straightforward to see that the high peaks represent 1s and the low peaks represent 0s. 
By setting some rules, a custom classifier could predict a result without the need to be trained with sample data.
In particular, the classifier analyses sequences of values, trying to find sequences that are either monotonically increasing or monotonically decreasing.
Also a noise factor has to be taken into account, in this way small enough variations are not considered.
As mentioned before, there are only two kinds of peaks in the sequences, either single or double peaks.
Therefore, it's of critical importance to be able to distinguish the two cases. One criteria that was originally deployed for this was to count the number of values that progress in the same direction, either up or down. This was later changed to the more precise criteria of duration of such sequences, and this is the reason why for each value a timestamp is included representing the time of reception for that specific value.\\ \todo{ stats about reception rate, std deviation.}\\
With this in mind, the algorithm has been developed to very simply count the duration of either monotonically increasing or decreasing sequences of values, and produce a prediction when the direction changes, based on such duration.
This technique performs particularly well in this application, producing up to 100\% success rate in certain cases.
 Runtime wise, the algorithm takes constant time for each value that that is received, and may or may not produce a result.
A simplified version of the algorithm can be found in fig. \ref{code}.
\begin{figure}
\centering
\begin{lstlisting}[language=Python, frame={}]
	def feed(self, time, value):
		pred = None
		
		# staying
		if abs(value - self.prev) <= self.epsilon:
			pass

		#going down
		elif value <= self.prev: 
			if self.direction: # up	
				pred = self._predict(time)

		# going up
		elif value > self.prev: 
			if not self.direction: # down
				pred = self._predict(time)

		self.prev = value
		return pred
		
	def _predict(self, time):
		m = self.direction
		delta = time - self.seqstart
		pred = '-'
		
		if delta >= self._doubletime:
			pred = '11' if m else '00'
		else:
			pred = '1' if m else '0'

		self.direction = not self.direction
		self.seqstart = time
		return pred
\end{lstlisting}
\caption{Simplified algorithm for signal interpretation (Python 2.7).}
\label{code}
\end{figure}

\subsection{Protocol}
As can be seen the rates of transmission and reception in this prototype system are not very high, if compared to average wireless transmission speeds, which are measured in Mbps.
Transmission in visible light is also exposed to a high degree of uncertainty, depending on parameters like distance, maximum brightness, interference, noise, and so on.
In this prototype, communication is only established in one direction, which additionally increases the treat of getting wrong results.
In order to compensate all this aspects, an effort to strengthen the success rate has been made by structuring the communication into a very basic protocol.
Each PDU is included between two single bytes: STX, that indicates the start of a message, and a byte ETX which indicates its end. These two are bytes 0x02 and 0x03 respectively. To avoid that the reader assumes an ETX byte wrongfully while reading the message, a length byte LEN is also included in the header of the PDU, to specify the length of the message in bytes.
Additionally, each PDU is restrained to have a maximum size, and longer messages need to be split into more PDUs.

\subsubsection{Additional fields}
Increasing the functionalities or complexity of the system, additional parameters in the PDU might result useful if not necessary. 
Imagining a scenario with multiple receivers of messages broadcasted through light in a unilateral way, a receiver field could be added to the PDU. Also, it would be wise to perform encryption on the PDUs, in a way that only the intended receivers would be able to read the message directed to them.
Establishing 2-way communication would also require more steps. 2-way communication would be a very good way to ensure correctness of the transmission. For this, at least 2 more parameters seem necessary: a sequence number to identify the packet, and a checksum to check correctness of the received message.
A header for visible light communication doesn't require many parameters since it's likely to be the very last end of a communication process. Since it's mainly short distanced, it doesn't need any routing parameters as it would in big networks. 


\subsection{Results}

\begin{figure}
\centering
\includegraphics[height=180px]{img/transmission}
\caption{Example of transmission of message "a", the transmission include STX, ETX, and LEN bytes.}
\label{fig:transmissionA}
\end{figure}

[Experimental setup, experiments, performance and the the results I achieved in the prototype.]\\
how many errors per total bits sent?\\
correctness rate for different distances\\
different bulbs might also change\\
compare with other forms of communication (here or somewhere else?)\\
Also compare the use of protocol vs not protocol\\